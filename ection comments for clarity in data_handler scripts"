[1mdiff --git a/dispaset/postprocessing/data_handler.py b/dispaset/postprocessing/data_handler.py[m
[1mindex 4047306..77eba73 100644[m
[1m--- a/dispaset/postprocessing/data_handler.py[m
[1m+++ b/dispaset/postprocessing/data_handler.py[m
[36m@@ -68,6 +68,8 @@[m [mdef GAMSstatus(statustype, num):[m
     :param num:     Indicated termination condition (Integer)[m
     :returns:       String with the status[m
     """[m
[32m+[m[41m    [m
[32m+[m[32m    # Section 1: Define Model Status Messages[m
     if statustype == "model":[m
         msg = {1: u'Optimal solution achieved',[m
                2: u'Local optimal solution achieved',[m
[36m@@ -88,6 +90,8 @@[m [mdef GAMSstatus(statustype, num):[m
                17: u'Singular in a CNS models',[m
                18: u'Unbounded \u2013 no solution',[m
                19: u'Infeasible \u2013 no solution'}[m
[32m+[m[41m        [m
[32m+[m[32m    # Section 2: Define Solver Status Messages[m
     elif statustype == "solver":[m
         msg = {1: u'Normal termination',[m
                2: u'Solver ran out of iterations (fix with iterlim)',[m
[36m@@ -102,8 +106,12 @@[m [mdef GAMSstatus(statustype, num):[m
                11: u'Solver terminated with some type of failure (see LST file)',[m
                12: u'Solver terminated with some type of failure (see LST file)',[m
                13: u'Solver terminated with some type of failure (see LST file)'}[m
[32m+[m[41m        [m
[32m+[m[32m    # Section 3: Handle Incorrect Status Type[m
     else:[m
         sys.exit('Incorrect GAMS status type')[m
[32m+[m[41m        [m
[32m+[m[32m    # Section 4: Return Status Message[m
     return str(msg[num])[m
 [m
 [m
[36m@@ -120,7 +128,8 @@[m [mdef get_sim_results(path, cache=None, temp_path=None, return_xarray=False, retur[m
                                 (inputs, results, status). The latter is a dictionary with diagnostic messages.[m
     :returns inputs,results:    Two dictionaries with all the input and outputs[m
     """[m
[31m-[m
[32m+[m[41m    [m
[32m+[m[32m    # Section 1: Load Input and Result Files[m
     inputfile = path + '/' + inputs_file[m
     resultfile = path + '/' + results_file[m
     if cache is not None or temp_path is not None:[m
[36m@@ -129,6 +138,7 @@[m [mdef get_sim_results(path, cache=None, temp_path=None, return_xarray=False, retur[m
 [m
     inputs = pd.read_pickle(inputfile)[m
 [m
[32m+[m[32m    # Section 2: Clean and Prepare Data[m
     # Clean power plant names:[m
     inputs['sets']['u'] = clean_strings(inputs['sets']['u'])[m
     inputs['units'].index = clean_strings(inputs['units'].index.tolist())[m
[36m@@ -137,6 +147,7 @@[m [mdef get_sim_results(path, cache=None, temp_path=None, return_xarray=False, retur[m
     if not 'param_df' in inputs:[m
         inputs['param_df'] = ds_to_df(inputs)[m
 [m
[32m+[m[32m    # Section 3: Get GAMS Path and Load Results[m
     # Get GAMS path from config or try to locate it[m
     gams_dir = get_gams_path(inputs.get('config', {}).get('GAMS_folder'))[m
     if not gams_dir:[m
[36m@@ -146,6 +157,7 @@[m [mdef get_sim_results(path, cache=None, temp_path=None, return_xarray=False, retur[m
     results = gdx_to_dataframe(gdx_to_list(gams_dir, resultfile, varname='all', verbose=True),[m
                                fixindex=True, verbose=True)[m
 [m
[32m+[m[32m    # Section 4: Set Datetime Index[m
     # Set datetime index:[m
     StartDate = inputs['config']['StartDate'][m
     StopDate = inputs['config']['StopDate']  # last day of the simulation with look-ahead period[m
[36m@@ -161,6 +173,8 @@[m [mdef get_sim_results(path, cache=None, temp_path=None, return_xarray=False, retur[m
             'LostLoad_RampDown', 'LostLoad_2D', 'ShadowPrice', 'StorageShadowPrice',[m
             'ShadowPrice_2U', 'ShadowPrice_2D', 'ShadowPrice_3U',[m
             'status']  # 'status'[m
[32m+[m[41m    [m
[32m+[m[32m    # Section 5: Process Results for Each Key[m
     # TODO: Check backward compatibility[m
     keys_sparse = ['OutputPower', 'OutputPowerConsumption', 'OutputSystemCost', 'OutputCommitted',[m
                    'OutputCurtailedPower', 'OutputFlow', 'OutputShedLoad', 'OutputSpillage', 'OutputStorageLevel',[m
[36m@@ -186,6 +200,7 @@[m [mdef get_sim_results(path, cache=None, temp_path=None, return_xarray=False, retur[m
                    'OutputSystemCostD', 'SMML-SystemMinusesMaximalLoad', 'SMNL-SystemMinusesNominalLoad',[m
                    'UnitHourly2URevenue', 'UnitHourlyProductionCost', 'UnitHourlyStartUpCost', 'UnitHourlyVariableCost'][m
 [m
[32m+[m[32m    # Section 6: Include Water Slack in Results[m
     # Setting the proper index to the result dataframes:[m
     for key in chain(keys, keys_sparse):[m
         if key in results:[m
[36m@@ -219,6 +234,8 @@[m [mdef get_sim_results(path, cache=None, temp_path=None, return_xarray=False, retur[m
 [m
         # Clean power plant names:[m
     results['OutputPower'].columns = clean_strings(results['OutputPower'].columns.tolist())[m
[32m+[m[41m    [m
[32m+[m[32m    # Section 7: Remove Epsilons and Clean Data[m
     # Remove epsilons:[m
     if 'ShadowPrice' in results:[m
         results['ShadowPrice'][results['ShadowPrice'] >= 1e300] = 0[m
[36m@@ -232,6 +249,7 @@[m [mdef get_sim_results(path, cache=None, temp_path=None, return_xarray=False, retur[m
     # Remove powerplants with no generation[m
     results['OutputPower'] = results['OutputPower'].loc[:, (results['OutputPower'] != 0).any(axis=0)][m
 [m
[32m+[m[32m    # Section 8: Calculate Additional Metrics[m
     # Total nodal power consumption[m
     results['NodalPowerConsumption'] = pd.DataFrame()[m
     for z in inputs['sets']['n']:[m
[36m@@ -261,6 +279,7 @@[m [mdef get_sim_results(path, cache=None, temp_path=None, return_xarray=False, retur[m
     results['SMML-SystemMinusesMaximalLoad'] = results['EENS-ExpectedEnergyNotServed'] / results[[m
         'TotalDemand'].max()[m
 [m
[32m+[m[32m    # Section 9: Handle Status and Return Results[m
     status = {}[m
     if "model" in results['status']:[m
         errors = results['status'][(results['status']['model'] != 1) & (results['status']['model'] != 8)][m
[36m@@ -339,21 +358,31 @@[m [mdef inputs_to_xarray(inputs):[m
 [m
     try:[m
         import xarray as xr[m
[32m+[m
[32m+[m[32m        # Section 1: Initialize Data Structures[m
         # build set dictionary {parameter:set}[m
         in_keys = {}[m
[32m+[m[41m        [m
[32m+[m[32m        # Section 2: Build Set Dictionary[m
         for k, v in inputs['parameters'].items():[m
             in_keys[k] = tuple(v['sets'])[m
[32m+[m[41m            [m
[32m+[m[32m        # Section 3: Remove Config and Store Metadata[m
         # Remove config from dict and add it later as dataset attribute (metadata)[m
         __ = in_keys.pop('Config')[m
         config = inputs['config'][m
         version = inputs.get('version', '')[m
         all_ds = [][m
[32m+[m[41m        [m
[32m+[m[32m        # Section 4: Iterate and Build Values and Coordinates[m
         # Iterate all and build values nad coordinates[m
         for k, v in in_keys.items():[m
             val = inputs['parameters'][k]['val'][m
             var_name = k[m
             ind = v[m
             coords = OrderedDict()[m
[32m+[m[41m            [m
[32m+[m[32m            # Section 4.1: Build Coordinates Dictionary[m
             # Build dictionary for the specific parameter {sets:set elements}[m
             for sett in ind:[m
                 coords[sett] = inputs['sets'][sett][m
[36m@@ -365,18 +394,23 @@[m [mdef inputs_to_xarray(inputs):[m
                               name=var_name)[m
 [m
             all_ds.append(ds)[m
[32m+[m[41m            [m
[32m+[m[32m        # Section 5: Merge DataArrays and Add Metadata[m
         inputs = xr.merge(all_ds)[m
         inputs.attrs['version'] = version[m
         for key in config:[m
             if isinstance(config[key], (float, int, str)):[m
                 inputs.attrs[key] = config[key][m
[32m+[m[41m                [m
[32m+[m[32m        # Section 6: Replace 'h' with DateTimeIndex[m
         # Replace h with DateTimeIndex[m
         StartDate = config['StartDate'][m
         StopDate = config['StopDate']  # last day of the simulation with look-ahead period[m
         StopDate_long = dt.datetime(*StopDate) + dt.timedelta(days=config['LookAhead'])[m
         index_long = pd.date_range(start=dt.datetime(*StartDate), end=StopDate_long, freq='h')[m
         inputs.coords['h'] = index_long[m
[31m-[m
[32m+[m[41m        [m
[32m+[m[32m    # Section 7: Handle Import Error[m
     except ImportError:[m
         logging.warn('Cannot find xarray package. Falling back to dict of dataframes')[m
     return inputs[m
[1mdiff --git a/dispaset/preprocessing/data_handler.py b/dispaset/preprocessing/data_handler.py[m
[1mindex 08f709e..d451d31 100644[m
[1m--- a/dispaset/preprocessing/data_handler.py[m
[1m+++ b/dispaset/preprocessing/data_handler.py[m
[36m@@ -31,9 +31,11 @@[m [mdef NodeBasedTable(varname, config, default=None):[m
     :return:           Dataframe with the time series for each unit[m
     """[m
 [m
[32m+[m[32m    # Section 1: Path Handling[m
     path = config[varname][m
     zones = config['zones'][m
     paths = {}[m
[32m+[m[41m    [m
     if os.path.isfile(path):[m
         paths['all'] = path[m
         SingleFile = True[m
[36m@@ -52,6 +54,8 @@[m [mdef NodeBasedTable(varname, config, default=None):[m
         logging.critical([m
             'A path has been specified for table ' + varname + ' (' + path + ') but no file has been found')[m
         sys.exit(1)[m
[32m+[m[41m        [m
[32m+[m[32m    # Section 2: Data Initialization[m
     data = pd.DataFrame(index=config['idx_long'])[m
     if len(paths) == 0:[m
         logging.info('No data file specified for the table ' + varname + '. Using default value ' + str(default))[m
[36m@@ -62,6 +66,8 @@[m [mdef NodeBasedTable(varname, config, default=None):[m
         else:[m
             logging.critical('Default value provided for table ' + varname + ' is not valid')[m
             sys.exit(1)[m
[32m+[m[41m            [m
[32m+[m[32m    # Section 3: Data Loading from Single File[m
     elif SingleFile:[m
         # If it is only one file, there is a header with the zone code[m
         tmp = load_time_series(config, paths['all'])[m
[36m@@ -89,6 +95,8 @@[m [mdef NodeBasedTable(varname, config, default=None):[m
                     else:[m
                         logging.critical('Default value provided for table ' + varname + ' is not valid')[m
                         sys.exit(1)[m
[32m+[m[41m                        [m
[32m+[m[32m    # Section 4: Data Loading from Multiple Files[m
     else:  # assembling the files in a single dataframe:[m
         for z in paths:[m
             # In case of separated files for each zone, there is no header[m
[36m@@ -120,6 +128,8 @@[m [mdef UnitBasedTable(plants, varname, config, fallbacks=['Unit'], default=None, Re[m
 [m
     :return:           Dataframe with the time series for each unit[m
     """[m
[32m+[m[41m    [m
[32m+[m[32m    # Section 1: Path Handling[m
     path = config[varname][m
     zones = config['zones'][m
     paths = {}[m
[36m@@ -141,6 +151,7 @@[m [mdef UnitBasedTable(plants, varname, config, fallbacks=['Unit'], default=None, Re[m
             'A path has been specified for table ' + varname + ' (' + path + ') but no file has been found')[m
         sys.exit(1)[m
 [m
[32m+[m[32m    # Section 2: Data Initialization[m
     data = pd.DataFrame(index=config['idx_long'])[m
     if len(paths) == 0:[m
         logging.info('No data file specified for the table ' + varname + '. Using default value ' + str(default))[m
[36m@@ -152,6 +163,8 @@[m [mdef UnitBasedTable(plants, varname, config, fallbacks=['Unit'], default=None, Re[m
             logging.critical('Default value provided for table ' + varname + ' is not valid')[m
             sys.exit(1)[m
     else:  # assembling the files in a single dataframe:[m
[32m+[m[41m        [m
[32m+[m[32m        # Section 3: Data Loading and Processing[m
         columns = [][m
         for z in paths:[m
             tmp = load_time_series(config, paths[z])[m
[36m@@ -202,6 +215,8 @@[m [mdef UnitBasedTable(plants, varname, config, fallbacks=['Unit'], default=None, Re[m
                     out = pd.concat([out, pd.DataFrame(index=out.index, columns=[u], dtype=float).fillna(default)], axis=1)[m
                     new_header.append(u)[m
         out.columns = new_header[m
[32m+[m[41m        [m
[32m+[m[32m    # Section 4: Validation[m
     if not out.columns.is_unique:[m
         logging.critical([m
             'The column headers of table "' + varname + '" are not unique!. The following headers are duplicated: ' +[m
[36m@@ -222,6 +237,8 @@[m [mdef GenericTable(headers, varname, config, default=None):[m
 [m
     :return:           Dataframe with the time series for each unit[m
     """[m
[32m+[m[41m    [m
[32m+[m[32m    # Section 1: Path Handling[m
     path = config[varname][m
     paths = {}[m
     if os.path.isfile(path):[m
[36m@@ -234,7 +251,8 @@[m [mdef GenericTable(headers, varname, config, default=None):[m
         logging.critical('A path has been specified for table ' + varname +[m
                          ' (' + path + ') but no file has been found')[m
         sys.exit(1)[m
[31m-[m
[32m+[m[41m        [m
[32m+[m[32m    # Section 2: Data Initialization[m
     data = pd.DataFrame(index=config['idx_long'])[m
     if len(paths) == 0:[m
         logging.info('No data file specified for the table ' + varname + '. Using default value ' + str(default))[m
[36m@@ -246,6 +264,8 @@[m [mdef GenericTable(headers, varname, config, default=None):[m
             logging.critical('Default value provided for table ' + varname + ' is not valid')[m
             sys.exit(1)[m
     else:  # assembling the files in a single dataframe:[m
[32m+[m[41m        [m
[32m+[m[32m        # Section 3: Data Loading and Processing[m
         data = load_time_series(config, paths['all'])[m
         # For each plant and each fallback key, try to find the corresponding column in the data[m
         out = pd.DataFrame(index=config['idx_long'], dtype=float)[m
[36m@@ -255,6 +275,7 @@[m [mdef GenericTable(headers, varname, config, default=None):[m
             else:[m
                 logging.info('No specific information was found for header ' + header + ' in table ' + varname +[m
                              '. Using default value ' + str(default))[m
[32m+[m[32m    # Section 4: Validation[m
     if not out.columns.is_unique:[m
         logging.critical('The column headers of table "' + varname +[m
                          '" are not unique!. The following headers are duplicated: ' +[m
[36m@@ -275,7 +296,8 @@[m [mdef merge_series(plants, oldplants, data, method='WeightedAverage', tablename=''[m
 [m
     :return merged:     Pandas dataframe with the merged time series when necessary[m
     """[m
[31m-    # backward compatibility:[m
[32m+[m[41m    [m
[32m+[m[32m    # Section 1: Backward Compatibility and Initialization[m
     if not "Nunits" in plants:[m
         plants['Nunits'] = 1[m
 [m
[36m@@ -285,12 +307,14 @@[m [mdef merge_series(plants, oldplants, data, method='WeightedAverage', tablename=''[m
 [m
     merged = pd.DataFrame(index=data.index)[m
 [m
[32m+[m[32m    # Section 2: Create Unit Mapping[m
     # Create a dictionary relating the former units to the new (clustered) ones:[m
     units = {}[m
     for u in plants.index:[m
         for uu in plants.loc[u, 'FormerUnits']:[m
             units[uu] = u[m
[31m-[m
[32m+[m[41m            [m
[32m+[m[32m    # Section 3: Data Validation[m
     # First check the data:[m
     if not isinstance(data, pd.DataFrame):[m
         logging.critical('The input "' + tablename + '" to the merge_series function must be a dataframe')[m
[36m@@ -299,6 +323,8 @@[m [mdef merge_series(plants, oldplants, data, method='WeightedAverage', tablename=''[m
         if str(data[key].dtype) not in ['bool', 'int', 'float', 'float16', 'float32', 'float64', 'float128', 'int8',[m
                                         'int16', 'int32', 'int64']:[m
             logging.critical('The column "' + str(key) + '" of table + "' + tablename + '" is not numeric!')[m
[32m+[m[41m            [m
[32m+[m[32m    # Section 4: Data Processing[m
     for key in data:[m
         if key in units:[m
             newunit = units[key][m
[36m@@ -385,11 +411,12 @@[m [mdef load_time_series(config, path, header='infer'):[m
     :param: header      list of header names[m
     :return:            reindexed timeseries[m
     """[m
[31m-[m
[32m+[m[32m    # Section 1: Load Data[m
     data = pd.read_csv(path, index_col=0, parse_dates=True, header=header, keep_default_na=False)[m
     # Replace empty strings with NaN before converting to float[m
     data = data.replace('', np.nan).astype(float)[m
[31m-[m
[32m+[m[41m    [m
[32m+[m[32m    # Section 2: Check Index Uniqueness and Monotonicity[m
     if not data.index.is_unique:[m
         logging.critical('The index of data file ' + path + ' is not unique. Please check the data')[m
         sys.exit(1)[m
[36m@@ -404,6 +431,7 @@[m [mdef load_time_series(config, path, header='infer'):[m
                                                                   'you use the proper american date format (yyyy-mm-dd hh:mm:ss)')[m
             sys.exit(1)[m
 [m
[32m+[m[32m    # Section 3: Handle Numerical Indexes[m
     # First convert numerical indexes into datetimeindex:[m
     if str(data.index.dtype).startswith(('int', 'float')):[m
         if len(data) == len(config['idx']):  # The data has the same length as the provided index range[m
[36m@@ -421,6 +449,7 @@[m [mdef load_time_series(config, path, header='infer'):[m
                                                                                    'allow guessing its timestamps. Please use a 8760 elements time series')[m
             sys.exit(1)[m
 [m
[32m+[m[32m    # Section 4: Process Datetime Index[m
     if data.index.inferred_type == 'datetime64':[m
         data.index = data.index.tz_localize(None)  # removing locational data[m
         main_year = data.groupby(data.index.year).size()[m
[36m@@ -471,6 +500,7 @@[m [mdef load_time_series(config, path, header='infer'):[m
         logging.critical('Index for file ' + path + ' is not valid')[m
         sys.exit(1)[m
 [m
[32m+[m[32m    # Section 5: Re-indexing and Return[m
     # re-indexing with the longer index (including look-ahead) and filling possibly missing data at the beginning and[m
     # at the end:[m
     return data.reindex(config['idx_long'], method='nearest').bfill().astype(float)[m
[36m@@ -528,6 +558,8 @@[m [mdef load_config_excel(ConfigFile, AbsPath=True):[m
     :param ConfigFile: String with (relative) path to the DispaSET excel configuration file[m
     :param AbsPath:    If true, relative paths are automatically changed into absolute paths (recommended)[m
     """[m
[32m+[m[41m    [m
[32m+[m[32m    # Section 1: Initial Setup[m
     import xlrd[m
     xlrd.xlsx.ensure_elementtree_imported(False, None)[m
     xlrd.xlsx.Element_has_iter = True[m
[36m@@ -536,6 +568,7 @@[m [mdef load_config_excel(ConfigFile, AbsPath=True):[m
     sheet = wb.sheet_by_name('main')[m
     config = {}[m
 [m
[32m+[m[32m    # Section 2: Configuration for Version 20.01[m
     if sheet.cell_value(0, 0) == 'Dispa-SET Configuration File (v20.01)':[m
         config['Description'] = sheet.cell_value(5, 1)[m
         config['StartDate'] = xlrd.xldate_as_tuple(sheet.cell_value(56, 2), wb.datemode)[m
[36m@@ -894,6 +927,8 @@[m [mdef load_config_excel(ConfigFile, AbsPath=True):[m
 def load_config_yaml(filename, AbsPath=True):[m
     """ Loads YAML file to dictionary"""[m
     import yaml[m
[32m+[m[41m    [m
[32m+[m[32m    # Section 1: Load YAML File[m
     with open(filename, 'r') as f:[m
         try:[m
             config = yaml.full_load(f)[m
[36m@@ -901,15 +936,16 @@[m [mdef load_config_yaml(filename, AbsPath=True):[m
             logging.error('Cannot parse config file: {}'.format(filename))[m
             raise exc[m
 [m
[32m+[m[32m    # Section 2: Add Default Parameters for Backward Compatibility[m
     # List of parameters to be added with a default value if not present (for backward compatibility):[m
[31m-[m
     params_to_be_added = {'DataTimeStep': 1, 'SimulationTimeStep': 1, 'HydroScheduling': 'Off',[m
                           'HydroSchedulingHorizon': 'Annual', 'InitialFinalReservoirLevel': True,[m
                           'ReserveParticipation_CHP': [], 'OptimalityGap': 0.005, 'CplexSetting': 'Default'}[m
     for param in params_to_be_added:[m
         if param not in config:[m
             config[param] = params_to_be_added[param][m
[31m-[m
[32m+[m[41m            [m
[32m+[m[32m    # Section 3: Set Non-Empty Default Values[m
     # Set default values (for backward compatibility):[m
     NonEmptyDefaultss = {'ReservoirLevelInitial': 0.5, 'ReservoirLevelFinal': 0.5, 'ValueOfLostLoad': 1E5,[m
                          'CostXSpillage': 1, 'WaterValue': 100, 'ShareOfQuickStartUnits': 0.5, 'CostCurtailment': 0}[m
[36m@@ -917,6 +953,7 @@[m [mdef load_config_yaml(filename, AbsPath=True):[m
         if param not in config['default']:[m
             config['default'][param] = NonEmptyDefaultss[param][m
 [m
[32m+[m[32m    # Section 4: Define Missing Parameters[m
     # Define missing parameters if they were not provided in the config file[m
     PARAMS = ['Demand', 'Outages', 'PowerPlantData', 'RenewablesAF', 'LoadShedding', 'NTC', 'Interconnections',[m
               'ReservoirScaledInflows', 'ReservoirScaledOutflows','PriceOfNuclear', 'PriceOfBlackCoal', 'PriceOfGas',[m
[36m@@ -930,11 +967,14 @@[m [mdef load_config_yaml(filename, AbsPath=True):[m
     for param in PARAMS:[m
         if param not in config:[m
             config[param] = ''[m
[32m+[m[41m            [m
[32m+[m[32m    # Section 5: Set Global Defaults[m
     global DEFAULTS[m
     for key in DEFAULTS:[m
         if key not in config['default']:[m
             config['default'][key] = DEFAULTS[key][m
[31m-[m
[32m+[m[41m            [m
[32m+[m[32m    # Section 6: Convert Relative Paths to Absolute Paths[m
     if AbsPath:[m
         # Changing all relative paths to absolute paths. Relative paths must be defined[m
         # relative to the parent folder of the config file.[m
[36m@@ -948,6 +988,8 @@[m [mdef load_config_yaml(filename, AbsPath=True):[m
                     config[param] = ''[m
                 elif not os.path.isabs(config[param]):[m
                     config[param] = os.path.join(basefolder, config[param])[m
[32m+[m[41m                    [m
[32m+[m[32m    # Section 7: Return Configuration[m
     return config[m
 [m
 [m
